{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liquid AI API\n",
    "\n",
    "This notebook demonstrate the rest of the api usages besides openai compatible apis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baisc Completion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add environment variables\n",
    "\n",
    "```bash\n",
    "export LIQUID_URL=\"https://labs.liquid.ai/api/v1\"\n",
    "export LIQUID_API_KEY=\"9cba1.....\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liquidai\n",
    "from getpass import getpass\n",
    "\n",
    "client = liquidai.Client(\n",
    "    base_url=\"https://labs.liquid.ai/api/v1\",\n",
    "    api_key=getpass(\"Enter your Liquid AI API Key: \"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Error: {\"detail\":\"Model liquid-beacon-1.0 not found\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the largest animal on earth?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliquid-beacon-1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Documents/projects/liquidaiapp/liquid_client/liquidai/client.py:122\u001b[0m, in \u001b[0;36mClient.complete\u001b[0;34m(self, messages, model, max_new_tokens, top_p, temperature, top_k)\u001b[0m\n\u001b[1;32m    109\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[1;32m    116\u001b[0m }\n\u001b[1;32m    117\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/complete\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m     json\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    120\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_for_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/projects/liquidaiapp/liquid_client/liquidai/client.py:30\u001b[0m, in \u001b[0;36mClient._check_for_errors\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_for_errors\u001b[39m(\u001b[38;5;28mself\u001b[39m, response):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour API Key was not accepted by the server. Make sure you have the correct API Key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Error: {\"detail\":\"Model liquid-beacon-1.0 not found\"}"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"system\\nYou are Baltazar, a friendly AI assistant trained by Liquid AI!\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the largest animal on earth?\"},\n",
    "]\n",
    "response = client.complete(\n",
    "    messages,\n",
    "    model=\"liquid-preview-0.1\",\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.9,\n",
    "    temperature=0.9,\n",
    "    top_k=0,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liquid-beacon-1.0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag with files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='test.txt'>\n",
      "Uploaded test.txt to test.txt\n"
     ]
    }
   ],
   "source": [
    "test_file = \"test.txt\"\n",
    "with open(test_file, \"w\") as f:\n",
    "    f.write(\"The name of the CEO of Liquid is Ramin Hasani.\")\n",
    "db_document = client.upload_file(test_file)\n",
    "print(f\"Uploaded {test_file} to {db_document['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: [{'id': 'cef74c44-dd66-468c-bdb2-468b9d29a538', 'name': 'test.txt', 'created_at': '2024-04-16T22:55:50.945196', 'size': 46}, {'id': '2bf14226-5eaa-45fd-912d-77725c10e4df', 'name': 'attention_is_all_you_need.pdf', 'created_at': '2024-04-16T22:04:54.817733', 'size': 2215244}, {'id': '44de4054-3531-499b-80ae-a5f5e6507cc0', 'name': 'liquid-ai.txt', 'created_at': '2024-04-16T22:04:43.590349', 'size': 36}]\n"
     ]
    }
   ],
   "source": [
    "files = client.list_files()\n",
    "print(f\"Files: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'message': {'role': 'assistant', 'content': 'The CEO of Liquid is Ramin Hasani.'}}\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Who is the CEO of Liquid?\", \"files\": [\"test.txt\"]}]\n",
    "response = client.complete(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted test.txt\n",
      "Files: [{'id': '2bf14226-5eaa-45fd-912d-77725c10e4df', 'name': 'attention_is_all_you_need.pdf', 'created_at': '2024-04-16T22:04:54.817733', 'size': 2215244}, {'id': '44de4054-3531-499b-80ae-a5f5e6507cc0', 'name': 'liquid-ai.txt', 'created_at': '2024-04-16T22:04:43.590349', 'size': 36}]\n"
     ]
    }
   ],
   "source": [
    "client.delete_file(test_file)\n",
    "print(f\"Deleted {test_file}\")\n",
    "files = client.list_files()\n",
    "print(f\"Files: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted test.txt locally\n"
     ]
    }
   ],
   "source": [
    "# Delete the file locally\n",
    "import os\n",
    "\n",
    "os.remove(test_file)\n",
    "print(f\"Deleted {test_file} locally\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
