William Yarnel Slack (August 1, 1816 – March 21, 1862) was an American lawyer, politician and military officer who fought for the Confederate States of America during the American Civil War. Born in Kentucky, Slack moved to Missouri as a child and later entered the legal profession. After serving in the Missouri General Assembly from 1842 to 1843, he fought as a captain in the United States Army for fourteen months during the Mexican–American War, beginning in 1846. He saw action at the Battle of Embudo Pass and the Siege of Pueblo de Taos. Returning to a legal career, Slack became influential in his local area. After the outbreak of the American Civil War in April 1861, Slack, who held pro-slavery views, supported the Confederate cause. When the Missouri State Guard (MSG) was formed the next month to oppose the Union Army, he was appointed as a brigadier general in the MSG's 4th Division. After participating in the Battle of Carthage in July, he fought in the Battle of Wilson's Creek on August 10. After a surprise Union attack, Slack's deployment of his division gave time for further Confederate States Army and MSG troops to deploy. Suffering a bad hip wound at Wilson's Creek, he was unable to rejoin his command until October. Along with other Missouri State Guard officers, Slack transferred to the Confederate States Army in late 1861 where he commanded a brigade with the rank of colonel. On March 7, 1862, during the Battle of Pea Ridge, Slack suffered another wound that was close to the injury he had received at Wilson's Creek. Infection set in, and he died on March 21. He was posthumously promoted to brigadier general in the Confederate army on April 17; the Confederate States Senate may not have known that he was dead at the time of the promotion.William Yarnel Slack was born on August 1, 1816, in Mason County, Kentucky. His father, John Slack, farmed and made pottery; his mother was Mary J. Caldwell Slack.[1] The historian Kenneth E. Burchett states that Slack had Quaker roots.[2] In 1819,[3] the family moved to Columbia in the Missouri Territory to pursue agricultural opportunities.[1] Slack's father became a justice of the peace and grew tobacco.[1] The younger Slack was educated in the Columbia area,[4] studying law under one J. B. Gordon,[3] but returned to Kentucky in 1837 to pursue a legal career. After returning to Columbia in 1839, he was admitted to the bar and relocated to Chillicothe[1] where he opened a law office, as there were more opportunities for new lawyers in that town.[3] According to historian Jeffery S. Prushankin, Slack gained a reputation for "coolness under pressure as well as for his honesty and integrity". Entering into politics in 1842 as a member of the Democratic Party,[1] he was elected to the Missouri General Assembly. His term ended in 1843.[5] Three years later he was part of Missouri's state constitutional convention. Slack married Mary E. Woodward in 1842 and they had two children during the 1840s.[1] Despite opposing war in general, Slack organized a company in 1846 for service in the Mexican–American War. The unit became part of the 2nd Missouri Mounted Volunteers, and Slack was elected captain of Company L.[6] The 2nd Missouri Volunteers served under Colonel Sterling Price,[7] and Slack was in the army for fourteen months.[4] Engaged in the fighting that took place in the Santa Fe area, Slack's conduct at the Battle of Embudo Pass in January 1847 gained praise from Price, and Slack's men blocked the enemy's retreat route in the Siege of Pueblo de Taos.[8] After his military service ended, Slack returned to Chillicothe and legal practice.[8] Continuing to be involved in politics, he was influential in his local area. Supporting slavery[9] and states' rights, he adhered to strict constructionism and opposed secession during the 1850s. Slack's wife died in 1858; he wed Isabella R. Bower the next year and they had two children. During the 1860 United States presidential election, he supported the candidacy of John C. Breckinridge. After Abraham Lincoln won the election in 1860, Slack considered that secession and war were likely.[6]
Nvidia Corporation[note 1][note 2] (/ɛnˈvɪdiə/, en-VID-ee-ə) is an American multinational corporation and technology company headquartered in Santa Clara, California, and incorporated in Delaware.[3] It is a software and fabless company which designs and supplies graphics processing units (GPUs), application programming interfaces (APIs) for data science and high-performance computing as well as system on a chip units (SoCs) for the mobile computing and automotive market. Nvidia is also a dominant supplier of artificial intelligence (AI) hardware and software.[4][5][6] Nvidia's professional line of GPUs are used for edge-to-cloud computing and in supercomputers and workstations for applications in such fields as architecture, engineering and construction, media and entertainment, automotive, scientific research, and manufacturing design.[7] Its GeForce line of GPUs are aimed at the consumer market and are used in applications such as video editing, 3D rendering and PC gaming. In the second quarter of 2023, Nvidia had a market share of 80.2% in the discrete desktop GPU market.[8] The company expanded its presence in the gaming industry with the introduction of the Shield Portable (a handheld game console), Shield Tablet (a gaming tablet) and Shield TV (a digital media player), as well as its cloud gaming service GeForce Now.[9] In addition to GPU design and manufacturing, Nvidia provides the CUDA software platform and API that allows the creation of massively parallel programs which utilize GPUs.[10][11] They are deployed in supercomputing sites around the world.[12][13] In the late 2000s, Nvidia had moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets as well as vehicle navigation and entertainment systems.[14][15][16] Its competitors include AMD, Intel,[17] Qualcomm[18] and AI accelerator companies such as Cerebras and Graphcore. It also makes AI-powered software for audio and video processing, e.g. Nvidia Maxine.[19] Nvidia's offer to acquire Arm from SoftBank in September 2020 failed to materialize following extended regulatory scrutiny, leading to the termination of the deal in February 2022 in what would have been the largest semiconductor acquisition.[20][21] In 2023, Nvidia became the seventh public U.S. company to be valued at over $1 trillion,[22] and, as of March 2024, it is the world's third most-valuable company after Microsoft and Apple, with a market capitalization of over $2 trillion.[23] Nvidia was founded on April 5, 1993,[24][25][26] by Jensen Huang (CEO as of 2024), a Taiwanese-American electrical engineer who was previously the director of CoreWare at LSI Logic and a microprocessor designer at AMD; Chris Malachowsky, an engineer who worked at Sun Microsystems; and Curtis Priem, who was previously a senior staff engineer and graphics chip designer at IBM and Sun Microsystems.[27][28] The three men founded the company in a meeting at a Denny's roadside diner in East San Jose (just off Interstate 680 at the Berryessa Road interchange).[29][30] In 1993, the three co-founders envisioned that the ideal trajectory for the forthcoming wave of computing would be in the realm of accelerated computing, specifically in graphics-based processing. This path was chosen due to its unique ability to tackle challenges that eluded general-purpose computing methods.[31] They also observed that video games were simultaneously one of the most computationally challenging problems and would have incredibly high sales volume; the two conditions do not happen very often.[31] Video games became the company's flywheel to reach large markets and fund huge R&D to solve massive computational problems.[31] With $40,000 in the bank, the company was born.[31] The company subsequently received $20 million of venture capital funding from Sequoia Capital and others.[32] Nvidia initially had no name and the co-founders named all their files NV, as in "next version".[31] The need to incorporate the company prompted the co-founders to review all words with those two letters, leading them to "invidia", the Latin word for "envy".[31] The company's original headquarters office was in Sunnyvale, California.[31] First graphics accelerator Nvidia's first graphics accelerator product, the NV1, was optimized for processing quadratic primitives instead of the triangle primitives preferred by its competitors.[30] Then Microsoft introduced the DirectX platform, refused to support any other graphics software,[33] and also announced that its graphics software (Direct3D) would support only triangles.[30] In 1996, Huang laid off more than half of Nvidia's employees (then around 100) and focused the company's remaining resources on developing a graphics accelerator product optimized for processing triangle primitives: the RIVA 128.[30][33] By the time the RIVA 128 was released in August 1997, Nvidia only had enough money left for about one month of payroll.[30] The sense of desperation around Nvidia during this difficult era of its early history gave rise to "the unofficial company motto": "Our company is thirty days from going out of business".[30] Nvidia sold about a million RIVA 128s in about four months[30] and used the revenue to develop its next generation of products.[33] In 1998, the release of the RIVA TNT solidified Nvidia's reputation for developing capable graphics adapters.Public company Nvidia went public on January 22, 1999.[34][35][36]In late 1999, Nvidia released the GeForce 256 (NV10), its first product expressly marketed as a GPU, which was most notable for introducing onboard transformation and lighting (T&L) to consumer-level 3D hardware. Running at 120 MHz and featuring four-pixel pipelines, it implemented advanced video acceleration, motion compensation, and hardware sub-picture alpha blending. The GeForce outperformed existing products by a wide margin.Due to the success of its products, Nvidia won the contract to develop the graphics hardware for Microsoft's Xbox game console, which earned Nvidia a $200 million advance. However, the project took many of its best engineers away from other projects. In the short term this did not matter, and the GeForce2 GTS shipped in the summer of 2000. In December 2000, Nvidia reached an agreement to acquire the intellectual assets of its one-time rival 3dfx, a pioneer in consumer 3D graphics technology leading the field from the mid-1990s until 2000.[37][38] The acquisition process was finalized in April 2002.[39]In 2001, Standard & Poor's selected Nvidia to replace the departing Enron in the S&P 500 stock index, meaning that index funds would need to hold Nvidia shares going forward.[40]In July 2002, Nvidia acquired Exluna for an undisclosed sum. Exluna made software-rendering tools and the personnel were merged into the Cg project.[41] In August 2003, Nvidia acquired MediaQ for approximately US$70 million.[42] On April 22, 2004, Nvidia acquired iReady, also a provider of high-performance TCP/IP and iSCSI offload solutions.[buzzword][43] In December 2004, it was announced that Nvidia would assist Sony with the design of the graphics processor (RSX) in the PlayStation 3 game console. On December 14, 2005, Nvidia acquired ULI Electronics, which at the time supplied third-party southbridge parts for chipsets to ATI, Nvidia's competitor.[44] In March 2006, Nvidia acquired Hybrid Graphics.[45] In December 2006, Nvidia, along with its main rival in the graphics industry AMD (which had acquired ATI), received subpoenas from the U.S. Department of Justice regarding possible antitrust violations in the graphics card industry.[46]
Mamba is a deep learning architecture focused on sequence modeling. It was developed by researchers from Carnegie Mellon University and Princeton University to address some limitations of transformer models, especially in processing long sequences, and it is based on the Structured State Space sequence (S4) model.[1][2][3]Architecture To enable handling long data sequences, Mamba incorporates the Structured State Space sequence model (S4).[1] S4 can effectively and efficiently model long dependencies by combining the strengths of continuous-time, recurrent, and convolutional models, enabling it to handle irregularly sampled data, have unbounded context, and remain computationally efficient both during training and testing.[4] Mamba, building on the S4 model, introduces significant enhancements, particularly in its treatment of time-variant operations. Central to its design is a unique selection mechanism that adapts structured state space model (SSM) parameters based on the input.[5][1] This enables Mamba to selectively focus on relevant information within sequences, effectively filtering out less pertinent data. The model transitions from a time-invariant to a time-varying framework, which impacts both the computation and efficiency of the system.[1][6] To address the computational challenges introduced by this time-variance, Mamba employs a hardware-aware algorithm. This algorithm enables efficient computation on modern hardware, like GPUs, by using kernel fusion, parallel scan, and recomputation.[1] The implementation avoids materializing expanded states in memory-intensive layers, thereby optimizing performance and memory usage. The result is an architecture that is significantly more efficient in processing long sequences compared to previous methods.[1][6] Additionally, Mamba simplifies its architecture by integrating the SSM design with MLP blocks, resulting in a homogeneous and streamlined structure, furthering the model's capability for general sequence modeling across various data types, including language, audio, and genomics, while maintaining efficiency in both training and inference.[1] Token-free language models: MambaByte Further information: Tokenization (lexical analysis) Operating on byte-sized tokens, transformers scale poorly as every token must "attend" to every other token leading to O(n2) scaling laws, as a result, Transformers opt to use subword tokenization in order to reduce the number of tokens in text, however this leads to very large vocabulary tables and word embeddings. This research investigates a novel approach to language modeling, MambaByte, which departs from the standard token-based methods. Unlike traditional models that rely on breaking text into discrete units, MambaByte directly processes raw byte sequences. This eliminates the need for tokenization, potentially offering several advantages:[8] Language Independence: Tokenization often relies on language-specific rules and vocabulary, limiting applicability across diverse languages. MambaByte's byte-level representation allows it to handle different languages without language-specific adaptations.Removes the bias of subword tokenisation: where common subwords are overrepresented and rare or new words are underrepresented or split into less meaningful units. This can affect the model's understanding and generation capabilities, particularly for languages with rich morphology or tokens not well-represented in the training data. Simplicity in Preprocessing: It simplifies the preprocessing pipeline by eliminating the need for complex tokenization and vocabulary management, reducing the preprocessing steps and potential errors. Subword tokenisation introduces a number of quirks in LLMs, such as failure modes where LLMs can't spell words, reverse certain words, handle rare tokens, which are not present in byte-level tokenisation.[9] Andrej Karpathy, a founder behind OpenAI has stated "Eternal glory to anyone who can delete tokenization as a required step in LLMs.[9] Mamba Mixture of Experts (MOE) Further information: Mixture of experts MoE Mamba represents a pioneering integration of the Mixture of Experts (MoE) technique with the Mamba architecture, enhancing the efficiency and scalability of State Space Models (SSMs) in language modeling. This model leverages the strengths of both MoE and SSMs, achieving significant gains in training efficiency—requiring 2.2 times fewer training steps than its predecessor, Mamba, while maintaining competitive performance. MoE Mamba showcases improved efficiency and effectiveness by combining selective state space modeling with expert-based processing, offering a promising avenue for future research in scaling SSMs to handle tens of billions of parameters. The model's design involves alternating Mamba and MoE layers, allowing it to efficiently integrate the entire sequence context and apply the most relevant expert for each token.[10][11] Vision Mamba Further information: Computer vision Vision Mamba (Vim) integrates SSMs with visual data processing, employing bidirectional Mamba blocks for visual sequence encoding. This method reduces the computational demands typically associated with self-attention in visual tasks. Tested on ImageNet classification, COCO object detection, and ADE20k semantic segmentation, Vim showcases enhanced performance and efficiency, capable of handling high-resolution images with lower computational resources. This positions Vim as a scalable model for future advancements in visual representation learning.[12] Impact and Future Directions Mamba LLM represents a significant potential shift in large language model architecture, offering faster, more efficient, and scalable models. Its potential impact is vast, including applications in real-time language translation, content generation, long-form text analysis, audio, and speech processing. Further research is ongoing to explore Mamba's capabilities and potential for even more diverse applications.
A transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper "Attention Is All You Need".[1] It has no recurrent units, and thus requires less training time than previous recurrent neural architectures, such as long short-term memory (LSTM),[2] and its later variation has been prevalently adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.[3] Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished. The transformer paper, published in 2017, is based on the softmax-based attention mechanism proposed by Bahdanau et. al. in 2014 for machine translation,[4][5] and the Fast Weight Controller, similar to a transformer, proposed in 1992.[6][7][8] This architecture is now used not only in natural language processing and computer vision,[9] but also in audio[10] and multi-modal processing. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs)[11] and BERT[12] (Bidirectional Encoder Representations from Transformers). In 1990, the Elman network, using a recurrent neural network, encoded each word in a training set as a vector, called a word embedding, and the whole vocabulary as a vector database, allowing it to perform such tasks as sequence-predictions that are beyond the power of a simple multilayer perceptron. A shortcoming of the static embeddings was that they didn't differentiate between multiple meanings of same-spelt words.[13] In 1992, the Fast Weight Controller was published by Jürgen Schmidhuber.[6] It learns to answer queries by programming the attention weights of another neural network through outer products of key vectors and value vectors called FROM and TO. The Fast Weight Controller was later shown to be equivalent to the unnormalized linear Transformer.[8][7][14][15] The terminology "learning internal spotlights of attention" was introduced in 1993.[16] In 1993, the IBM alignment models were used for statistical machine translation.[17] In 1997, a precursor of large language model, using recurrent neural networks, such as long short-term memory, was proposed. In 2001, a one-billion-word large text corpus, scraped from the Internet, referred to as "very very large" at the time, was used for word disambiguation.[18] In 2012, AlexNet demonstrated the effectiveness of large neural networks for image recognition, encouraging large artificial neural networks approach instead of older, statistical approaches. In 2014, a 380M-parameter seq2seq model for machine translation using two Long short-term Memory (LSTMs) networks was proposed by Sutskever at al.[19] The architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. In 2014, gating proved to be useful in a 130M-parameter seq2seq model, which used a simplified gated recurrent units (GRUs). Bahdanau et al[20] showed that GRUs are neither better nor worse than gated LSTMs.[21][22] In 2014, Bahdanau et al.[23] improved the previous seq2seq model by using an "additive" kind of attention mechanism in-between two LSTM networks. It was, however, not yet the parallelizable (scaled "dot product") kind of attention, later proposed in the 2017 transformer paper. In 2015, the relative performance of Global and Local (windowed) attention model architectures were assessed by Luong et al, a mixed attention architecture found to improve on the translations offered by Bahdanau's architecture, while the use of a local attention architecture reduced translation time.[24] In 2016, Google Translate gradually replaced the older statistical machine translation approach with the newer neural-networks-based approach that included a seq2seq model combined by LSTM and the "additive" kind of attention mechanism. They achieved a higher level of performance than the statistical approach, which took ten years to develop, in only nine months.[25][26] In 2017, the original (100M-sized) encoder-decoder transformer model with a faster (parallelizable or decomposable) attention mechanism was proposed in the "Attention is all you need" paper. As the model had difficulties converging, it was suggested that the learning rate should be linearly scaled up from 0 to maximal value for the first part of the training (i.e. 2% of the total number of training steps). The intent of the transformer model is to take a seq2seq model and remove its recurrent neural networks, but preserve its additive attention mechanism.[1] In 2018, in the ELMo paper, an entire sentence was processed before an embedding vector was assigning to each word in the sentence. A bi-directional LSTM was used to calculate such, deep contextualized embeddings for each word, improving upon the line of research from bag of words and word2vec. In 2018, an encoder-only transformer was used in the (more than 1B-sized) BERT model, improving upon ELMo.[27] In 2020, vision transformer[28] and speech-processing convolution-augmented transformer[29] outperformed recurrent neural networks, previously used for vision and speech. In 2020, difficulties with converging the original transformer were solved by normalizing layers before (instead of after) multiheaded attention by Xiong et al. This is called pre-LN Transformer.[30] In 2023, uni-directional ("autoregressive") transformers were being used in the (more than 100B-sized) GPT-3 and other OpenAI GPT models.[31][32] Predecessors Before transformers, predecessors of attention mechanism were added to gated recurrent neural networks, such as LSTMs and gated recurrent units (GRUs), which processed datasets sequentially. Dependency on previous token computations prevented them from being able to parallelize the attention mechanism. In 1992, fast weight controller was proposed as an alternative to recurrent neural networks that can learn "internal spotlights of attention".[16][6] In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens. The performance of old models was enhanced by adding an attention mechanism, which allowed a model to access any preceding point along the sequence. The attention layer weighs all previous states according to a learned measure of relevance, providing relevant information about far-away tokens. This proved to be especially useful in language translation, where far-away context can be essential for the meaning of a word in a sentence. The state vector has been accessible only after the last English word was processed while, for example, translating it from French by a LSTM model. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. If an attention mechanism is added, the decoder is given access to the state vectors of every input word, not just the last, and can learn attention weights that dictate how much to attend to each input state vector. The augmentation of seq2seq models with the attention mechanism was first implemented in the context of machine translation by Bahdanau, Cho, and Bengio in 2014.[4][5] Decomposable attention In 2016, highly parallelizable decomposable attention was successfully combined with a feedforward network.[33] This indicated that attention mechanisms were powerful in themselves and that sequential recurrent processing of data was not necessary to achieve the quality gains of recurrent neural networks with attention. In 2017, Vaswani et al. also proposed replacing recurrent neural networks with self-attention and started the effort to evaluate that idea.[1] Transformers, using an attention mechanism, processing all tokens simultaneously, calculated "soft" weights between them in successive layers. Since the attention mechanism only uses information about other tokens from lower layers, it can be computed for all tokens in parallel, which leads to improved training speed.
From Wikipedia, the free encyclopedia"Lithium-ion" redirects here. For the metal element, see Lithium."Liion" redirects here. Not to be confused with Lion.Lithium-ion batteryA 3.6 V Li-ion battery from a Nokia 3310 mobile phoneSpecific energy	100–265 Wh/kg (0.360–0.954 MJ/kg)[1][2]Energy density	250–693 Wh/L (0.90–2.49 MJ/L)[3][4]Specific power	c. 250–340 W/kg[1]Charge/discharge efficiency	80–90%[5]Energy/consumer-price	7.6 Wh/US$ (US$132/kWh)[6]Self-discharge rate	0.35% to 2.5% per month depending on state of charge[7]Cycle durability	400–1,200 cycles [8]Nominal cell voltage	3.6 / 3.7 / 3.8 / 3.85 V, LiFePO4 3.2 V, Li4Ti5O12 2.3 VA lithium-ion or Li-ion battery is a type of rechargeable battery that uses the reversible intercalation of Li+ ions into electronically conducting solids to store energy. In comparison with other commercial rechargeable batteries, Li-ion batteries are characterized by higher specific energy, higher energy density, higher energy efficiency, a longer cycle life, and a longer calendar life. Also noteworthy is a dramatic improvement in lithium-ion battery properties after their market introduction in 1991: within the next 30 years, their volumetric energy density increased threefold while their cost dropped tenfold.[9] The invention and commercialization of Li-ion batteries may have had one of the greatest impacts of all technologies in human history,[10] as recognized by the 2019 Nobel Prize in Chemistry. More specifically, Li-ion batteries enabled portable consumer electronics, laptop computers, cellular phones, and electric cars, or what has been called the e-mobility revolution.[11] It also sees significant use for grid-scale energy storage as well as military and aerospace applications.Lithium-ion cells can be manufactured to optimize energy or power density.[12] Handheld electronics mostly use lithium polymer batteries (with a polymer gel as an electrolyte), a lithium cobalt oxide (LiCoO 2) cathode material, and a graphite anode, which together offer high energy density.[13][14] Lithium iron phosphate (LiFePO4), lithium manganese oxide (LiMn2O4 spinel, or Li2MnO3-based lithium-rich layered materials, LMR-NMC), and lithium nickel manganese cobalt oxide (LiNiMnCoO2 or NMC) may offer longer life and a higher discharge rate. NMC and its derivatives are widely used in the electrification of transport, one of the main technologies (combined with renewable energy) for reducing greenhouse gas emissions from vehicles.[15] M. Stanley Whittingham conceived intercalation electrodes in the 1970s and created the first rechargeable lithium-ion battery, based on a titanium disulfide cathode and a lithium-aluminum anode, although it suffered from safety problems and was never commercialized.[16] John Goodenough expanded on this work in 1980 by using lithium cobalt oxide as a cathode.[17] The first prototype of the modern Li-ion battery, which uses a carbonaceous anode rather than lithium metal, was developed by Akira Yoshino in 1985 and commercialized by a Sony and Asahi Kasei team led by Yoshio Nishi in 1991.[18] M. Stanley Whittingham, John Goodenough, and Akira Yoshino were awarded the 2019 Nobel Prize in Chemistry for their contributions to the development of lithium-ion batteries.Lithium-ion batteries can be a safety hazard if not properly engineered and manufactured because they have flammable electrolytes that, if damaged or incorrectly charged, can lead to explosions and fires. Much progress has been made in the development and manufacturing of safe lithium-ion batteries.[19] Lithium-ion solid-state batteries are being developed to eliminate the flammable electrolyte. Improperly recycled batteries can create toxic waste, especially from toxic metals, and are at risk of fire. Moreover, both lithium and other key strategic minerals used in batteries have significant issues at extraction, with lithium being water intensive in often arid regions and other minerals often being conflict minerals such as cobalt. Both environmental issues have encouraged some researchers to improve mineral efficiency and find alternatives such as iron-air batteries.Research areas for lithium-ion batteries include extending lifetime, increasing energy density, improving safety, reducing cost, and increasing charging speed,[20][21] among others. Research has been under way in the area of non-flammable electrolytes as a pathway to increased safety based on the flammability and volatility of the organic solvents used in the typical electrolyte. Strategies include aqueous lithium-ion batteries, ceramic solid electrolytes, polymer electrolytes, ionic liquids, and heavily fluorinated systems.[22][23][24][25] HistoryMain article: History of the lithium-ion batteryResearch on rechargeable Li-ion batteries dates to the 1960s; one of the earliest examples is a CuF2/Li battery developed by NASA in 1965. The breakthrough that produced the earliest form of the modern Li-ion battery was made by British chemist M. Stanley Whittingham in 1974, who first used titanium disulfide (TiS 2) as a cathode material, which has a layered structure that can take in lithium ions without significant changes to its crystal structure. Exxon tried to commercialize this battery in the late 1970s, but found the synthesis expensive and complex, as TiS 2 is sensitive to moisture and releases toxic H2S gas on contact with water. More prohibitively, the batteries were also prone to spontaneously catch fire due to the presence of metallic lithium in the cells. For this, and other reasons, Exxon discontinued the development of Whittingham's lithium-titanium disulfide battery.[26] In 1980, working in separate groups Ned A. Godshall et al.,[27][28][29] and, shortly thereafter, Koichi Mizushima and John B. Goodenough, after testing a range of alternative materials, replaced TiS 2 with lithium cobalt oxide (LiCoO2, or LCO), which has a similar layered structure but offers a higher voltage and is much more stable in air. This material would later be used in the first commercial Li-ion battery, although it did not, on its own, resolve the persistent issue of flammability.[26] These early attempts to develop rechargeable Li-ion batteries used lithium metal anodes, which were ultimately abandoned due to safety concerns, as lithium metal is unstable and prone to dendrite formation, which can cause short-circuiting. The eventual solution was to use an intercalation anode, similar to that used for the cathode, which prevents the formation of lithium metal during battery charging. A variety of anode materials were studied. In 1980, Rachid Yazami demonstrated reversible electrochemical intercalation of lithium in graphite,[30][31] a concept originally proposed by Jürgen Otto Besenhard in 1974 but considered unfeasible due to unresolved incompatibilities with the electrolytes then in use.[26][32][33] In fact, Yazami's work was itself limited to a solid electrolyte (polyethylene oxide), because liquid solvents tested by him and before co-intercalated with Li+ ions into graphite, causing the graphite to crumble.In 1985, Akira Yoshino at Asahi Kasei Corporation discovered that petroleum coke, a less graphitized form of carbon, can reversibly intercalate Li-ions at a low potential of ~0.5 V relative to Li+ /Li without structural degradation.[34] Its structural stability originates from the amorphous carbon regions in petroleum coke serving as covalent joints to pin the layers together. Although the amorphous nature of petroleum coke limits capacity compared to graphite (~Li0.5C6, 0.186 Ah g–1), it became the first commercial intercalation anode for Li-ion batteries owing to its cycling stability.in 1987, Akira Yoshino patented what would become the first commercial lithium-ion battery using an anode of "soft carbon" (a charcoal-like material) along with Goodenough's previously reported LiCoO2 cathode and a carbonate ester-based electrolyte. This battery is assembled in a discharged state, which makes its manufacturing safer and cheaper. In 1991, using Yoshino's design, Sony began producing and selling the world's first rechargeable lithium-ion batteries. The following year, a joint venture between Toshiba and Asashi Kasei Co. also released their lithium-ion battery.[26] Significant improvements in energy density were achieved in the 1990s by replacing the soft carbon anode first with hard carbon and later with graphite. In 1990, Jeff Dahn and two colleagues at Dalhousie University (Canada) reported reversible intercalation of lithium ions into graphite in the presence of ethylene carbonate solvent (which is solid at room temperature and is mixed with other solvents to make a liquid), thus finding the final piece of the puzzle leading to the modern lithium-ion battery.[35] In 2010, global lithium-ion battery production capacity was 20 gigawatt-hours.[36] By 2016, it was 28 GWh, with 16.4 GWh in China.[37] Global production capacity was 767 GWh in 2020, with China accounting for 75%.[38] Production in 2021 is estimated by various sources to be between 200 and 600 GWh, and predictions for 2023 range from 400 to 1,100 GWh.[39] In 2012, John B. Goodenough, Rachid Yazami and Akira Yoshino received the 2012 IEEE Medal for Environmental and Safety Technologies for developing the lithium-ion battery; Goodenough, Whittingham, and Yoshino were awarded the 2019 Nobel Prize in Chemistry "for the development of lithium-ion batteries".[40] Jeff Dahn received the ECS Battery Division Technology Award (2011) and the Yeager award from the International Battery Materials Association (2016).In April 2023, CATL announced that it would begin scaled-up production of its semi-solid condensed matter battery that produces a then record 500 Wh/kg. They use electrodes made from a gelled material, requiring fewer binding agents. This in turn shortens the manufacturing cycle. One potential application is in battery-powered airplanes.[41][42][43] Another new development of lithium-ion batteries are flow batteries with redox-targeted solids,that use no binders or electron-conducting additives, and allow for completely independent scaling of energy and power.[44] DesignCylindrical Panasonic 18650 lithium-ion cell before closing.Lithium-ion battery monitoring electronics (over-charge and deep-discharge protection)Left: AA alkaline battery. Right: 18650 lithium ion batteryGenerally, the negative electrode of a conventional lithium-ion cell is graphite made from carbon. The positive electrode is typically a metal oxide or phosphate. The electrolyte is a lithium salt in an organic solvent.[45] The negative electrode (which is the anode when the cell is discharging) and the positive electrode (which is the cathode when discharging) are prevented from shorting by a separator.[46] The electrodes are separated from external electronics with a piece of metal called a current collector.[47] The negative and positive electrodes swap their electrochemical roles (anode and cathode) when the cell is charged. Despite this, in discussions of battery design the negative electrode of a rechargeable cell is often just called "the anode" and the positive electrode "the cathode".In its fully lithiated state of LiC6, graphite correlates to a theoretical capacity of 1339 coulombs per gram (372 mAh/g).[48] The positive electrode is generally one of three materials: a layered oxide (such as lithium cobalt oxide), a polyanion (such as lithium iron phosphate) or a spinel (such as lithium manganese oxide).[49] More experimental materials include graphene-containing electrodes, although these remain far from commercially viable due to their high cost.[50] Lithium reacts vigorously with water to form lithium hydroxide (LiOH) and hydrogen gas. Thus, a non-aqueous electrolyte is typically used, and a sealed container rigidly excludes moisture from the battery pack. The non-aqueous electrolyte is typically a mixture of organic carbonates such as ethylene carbonate and propylene carbonate containing complexes of lithium ions.[51] Ethylene carbonate is essential for making solid electrolyte interphase on the carbon anode,[52] but since it is solid at room temperature, a liquid solvent (such as propylene carbonate or diethyl carbonate) is added.The electrolyte salt is almost always lithium hexafluorophosphate (LiPF6), which combines good ionic conductivity with chemical and electrochemical stability. The hexafluorophosphate anion is essential for passivating the aluminum current collector used for the positive electrode. A titanium tab is ultrasonically welded to the aluminum current collector. Other salts like lithium perchlorate (LiClO 4), lithium tetrafluoroborate (LiBF4), and lithium bis(trifluoromethanesulfonyl)imide (LiC2F6NO4S2) are frequently used in research in tab-less coin cells, but are not usable in larger format cells,[53] often because they are not compatible with the aluminum current collector. Copper (with a spot-welded nickel tab) is used as the current collector at the negative electrode.Current collector design and surface treatments may take various forms: foil, mesh, foam (dealloyed), etched (wholly or selectively), and coated (with various materials) to improve electrical characteristics.[47] Depending on materials choices, the voltage, energy density, life, and safety of a lithium-ion cell can change dramatically. Current effort has been exploring the use of novel architectures using nanotechnology to improve performance. Areas of interest include nano-scale electrode materials and alternative electrode structures.[54]
rom Wikipedia, the free encyclopediaNot to be confused with Illbient or Chillhop.This article has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)This article possibly contains original research. (March 2018)This article may need to be rewritten to comply with Wikipedia's quality standards. (March 2018)Trip hopStylistic originsHip hopelectronicareggaepsychedeliasouldubbreakbeatrockfunkR&Bjazzskaalternative rock[1][2]loungeCultural origins	c. late 1980s – early 1990s, Bristol, EnglandDerivative formsIllbientpost-trip hopIDMRegional scenesUnited KingdomAustraliaOther topicsBristol underground sceneexperimental hip hopneo soulnu jazzacid jazzchill-out musicpsychedelic rapTrip hop is a musical genre that originated in the late 1980s in the United Kingdom, especially Bristol.[3] It has been described as a psychedelic fusion of hip hop and electronica with slow tempos and an atmospheric sound,[4][5][6] often incorporating elements of jazz, soul, funk, reggae, dub, R&B, and other forms of electronic music, as well as sampling from movie soundtracks and other eclectic sources.[7] The style emerged as a more experimental variant of breakbeat from the Bristol sound scene of the late 1980s and early 1990s,[8] incorporating influences from jazz, soul, funk, dub, and rap music.[9] It was pioneered by acts like Massive Attack, Tricky, and Portishead.[10] The term was first coined in a 1994 Mixmag piece about American producer DJ Shadow.[11] Trip hop achieved commercial success in the 1990s, and has been described as "Europe's alternative choice in the second half of the '90s".[8] CharacteristicsCommon musical aesthetics include a bass-heavy drumbeat,[12] often providing the slowed down breakbeat samples similar to standard 1990s hip hop beats, giving the genre a more psychedelic and mainstream feel.[12] Vocals in trip hop are often female and feature characteristics of various singing styles including R&B, jazz and rock. The female-dominant vocals of trip hop may be partially attributable to the influence of genres such as jazz and early R&B, in which female vocalists were more common. However, there are notable exceptions: Massive Attack[13] and Groove Armada collaborated with male and female vocalists, Tricky often features vocally in his own productions along with Martina Topley-Bird,[14] and Chris Corner provided vocals for later albums with Sneaker Pimps.[15] Trip hop is also known for its melancholic sound. This may be partly due to the fact that several acts were inspired by post-punk bands;[16] Tricky and Massive Attack both covered and sampled songs of Siouxsie and the Banshees[17][18] and the Cure.[19][20] Tricky opened his second album Nearly God with a version of "Tattoo", a proto-trip-hop song of Siouxsie and the Banshees initially recorded in 1983.[21] Trip hop tracks often incorporate Rhodes pianos, saxophones, trumpets, flutes, and may employ unconventional instruments such as the theremin and Mellotron. Trip hop differs from hip hop in theme and overall tone. Contrasting with gangsta rap and its hard-hitting lyrics, trip hop offers a more aural atmospherics influenced by experimental folk and rock acts of the seventies, such as John Martyn,[22] combined with instrumental hip hop, turntable scratching, and breakbeat rhythms. Regarded in some ways as a 1990s update of fusion, trip hop may be said to "transcend" the hardcore rap styles and lyrics with atmospheric overtones to create a more mellow tempo.[23] HistoryLate 1980s–1991: OriginsThe term "trip-hop" first appeared in print in June 1994.[24] Andy Pemberton, a music journalist writing for Mixmag, used it to describe "In/Flux", a single by American producer DJ Shadow and UK act RPM, with the latter signed to Mo' Wax Records.[25][26] In Bristol, hip hop began to seep into the consciousness of a subculture already well-schooled in Jamaican forms of music. DJs, MCs, b-boys and graffiti artists grouped together into informal soundsystems.[27] Like the pioneering Bronx crews of DJs Kool Herc, Afrika Bambataa and Grandmaster Flash, the soundsystems provided party music for public spaces, often in the economically deprived council estates from which some of their members originated. Bristol's soundsystem DJs, drawing heavily on Jamaican dub music, typically used a laid-back, slow and heavy drum beat ("down tempo").Bristol's Wild Bunch crew became one of the soundsystems to put a local spin on the international phenomenon, helping to birth Bristol's signature sound of trip hop, often termed "the Bristol Sound".[27] The Wild Bunch and its associates included at various times in its existence the MC Adrian "Tricky Kid" Thaws, the graffiti artist and lyricist Robert "3D" Del Naja, producer Jonny Dollar and the DJs Nellee Hooper, Andrew "Mushroom" Vowles and Grant "Daddy G" Marshall. As the hip hop scene matured in Bristol and musical trends evolved further toward acid jazz and house in the late 1980s,[28] the golden era of the soundsystem began to end. The Wild Bunch signed a record deal and evolved into Massive Attack,[29] a core collective of 3D, Mushroom and Daddy G, with significant contributions from Tricky Kid (soon shortened to Tricky), Dollar, and Hooper on production duties, along with a rotating cast of other vocalists.[29] Another influence came from Gary Clail's Tackhead soundsystem. Clail often worked with former The Pop Group singer Mark Stewart.[30] The latter experimented with his band Mark Stewart & the Maffia, which consisted of New York session musicians Skip McDonald, Doug Wimbish, and Keith LeBlanc, who had been a part of the house band for the Sugarhill Records record label.[31] Produced by Adrian Sherwood, the music combined hip hop with experimental rock and dub and sounded like a premature version of what later became trip hop. In 1993, Kirsty MacColl released "Angel", one of the first examples of the genre crossing over to pop, a hybrid that dominated the charts toward the end of the 1990s.1991–1997: Mainstream breakthrough"Teardrop"Duration: 29 seconds.0:29Sample of "Teardrop" by Massive Attack, from MezzanineProblems playing this file? See media help.Massive Attack, a British trip hop group that helped bring the genre to mainstream success in the 1990s[32]Massive Attack's first album Blue Lines was released in 1991 to huge success in the United Kingdom.[33] Blue Lines was seen widely as the first major manifestation of a uniquely British hip hop movement, but the album's hit single "Unfinished Sympathy" and other tracks were not seen as hip hop songs in a conventional sense despite similarities in production methods such as using sample-based rhythms. Produced by Dollar, Shara Nelson (R&B singer) featured on the orchestral "Unfinished", and Jamaican dance hall star Horace Andy provided vocals on several other tracks, as he would throughout Massive Attack's career.[34] Massive Attack released their second album entitled Protection in 1994. Although Tricky stayed on in a lesser role and Hooper again produced, the fertile dance music scene of the early 1990s had informed the record, and it was seen as an even more significant shift away from the Wild Bunch era.In the June 1994 issue of the UK magazine Mixmag, music journalist Andy Pemberton used the term trip hop to describe the hip hop instrumental "In/Flux", a 1993 single by San Francisco's DJ Shadow, and other similar tracks released on the Mo' Wax label and being played in London clubs at the time. "In/Flux", with its mixed up bpms, spoken word samples, strings, melodies, bizarre noises, prominent bass, and slow beats, gave the listener the impression they were on a musical trip, according to Pemberton.[35] Soon, however, Massive Attack's dubby, jazzy, psychedelic, electronic textures, rooted in hip hop sampling technique but taking flight into many styles, were described by journalists as the template of the eponymous genre.Tricky, a major trip hop artistIn 1993, Icelandic musician Björk released Debut, produced by Wild Bunch member Nellee Hooper.[36] The album, although rooted in four-on-the-floor house music, contained elements of trip hop and is credited as one of the first albums to introduce electronic dance music into mainstream pop.[37][38] She had been in contact with London's underground electronic music scene and was romantically involved with trip-hop musician Tricky. Björk further embraced trip-hop with her 1995 album Post by collaborating with Tricky and Howie B. Homogenic, her 1997 album, has been described as a pinnacle of trip hop music.[39] Trip-hop neared the peak of its popularity in 1994 and 1995, with artists such as Howie B and Earthling making significant contributions. Ninja Tune, the independent record label founded by the duo Coldcut, significantly influenced the trip-hop sound in London and beyond with breakthrough artists DJ Food, 9 Lazy 9, Up, Bustle & Out, Funki Porcini and The Herbaliser, among others. The period also marked the debut of two acts who, along with Massive Attack, would define the Bristol scene for years to come.In 1994, Portishead, a trio comprising singer Beth Gibbons, Geoff Barrow, and Adrian Utley, released their debut album Dummy. Their background differed from Massive Attack in many ways: one of Portishead's primary influences was 1960s and 1970s film soundtrack LPs.[40] Nevertheless, Portishead shared the scratchy, jazz-sample-based aesthetic of early Massive Attack (whom Barrow had briefly worked with during the recording of Blue Lines), and the sullen, fragile vocals of Gibbons also brought them wide acclaim. In 1995, Dummy was awarded the Mercury Music Prize as the best British album of the year,[41] giving trip-hop as a genre its greatest exposure yet. Portishead's music was also widely imitated, to the point that they distanced themselves[42] from the trip-hop label they had inadvertently helped popularize, with Barrow stating "The whole trip-hop tag was nonsense. It was developed by people in London, and the people in Bristol just had to put up with it.".[43] Tricky also released his debut solo album, Maxinquaye in 1995, to great critical acclaim. The album was produced largely in collaboration with Mark Saunders. Tricky employed whispered, often abstract stream-of-consciousness lyrics, remote from the gangsta-rap braggadocio of the mid-1990s US hip-hop scene. Even more unusually, many of the solo songs on Maxinquaye featured little of Tricky's own voice: his then-lover, Martina Topley-Bird, sang them, including her re-imagining of rap group Public Enemy's 1988 song "Black Steel in the Hour of Chaos", while other songs were male-female duets dealing with sex and love in oblique ways, over beds of sometimes dissonant samples. Within a year, Tricky had released two more full-length albums, although they failed to find the same popularity as his Bristol contemporaries Massive Attack and Portishead.[44] Through his collaborations with Björk, however, he exerted influence closer to the pop and alternative rock mainstream, and he developed a large cult fan-base.Although not as popular in the United States, bands like Portishead and Sneaker Pimps saw moderate airplay on alternative-rock stations across the country.[45] 1997–2010: Continued success and new directionsBjörk, an artist who has often incorporated trip hop in her music[46][47][48]After the initial success of trip hop in the mid-1990s, the artists who made their own interpretations of the genre include Archive, Baby Fox, Bowery Electric, Esthero, Morcheeba, Sneaker Pimps, Anomie Belle,[49] Alpha, Jaianto, Mudville and Cibo Matto and Lamb. These artists incorporated trip hop into other genres, including ambient, soul, IDM, industrial, dubstep, breakbeat, drum and bass, acid jazz, and new-age. The first printed use of the term "post-trip hop" was in an October 2002 article of The Independent, and was used to describe the band Second Person.[50] Trip hop has also influenced artists in other genres, including Gorillaz, Emancipator, Nine Inch Nails, Travis, PJ Harvey,[51] How to Destroy Angels,[52] Beth Orton, The Flaming Lips, Bitter:Sweet, Beck, The xx and Deftones.[53] Several tracks on Australian pop singer Kylie Minogue's 1997 album Impossible Princess also displayed a trip hop influence.[54][importance of example(s)?] Various prominent artists and groups, such as Janet Jackson,[55] Kylie Minogue,[56] Madonna,[57][58] Björk,[59][60][61] and Radiohead,[62] have also been influenced by the genre. Trip hop has spawned several subgenres,[63] including illbient (dub-based trip hop which combines ambient and industrial hip hop).Trip hop continued to influence notable artists in the 2000s. Japanese composer Akira Yamaoka, Norwegian avant-garde band Ulver incorporated trip hop in their ambient/electronic/jazzy album Perdition City. Atmospheric rock band Antimatter included some trip hop elements in their first two albums. Australian composer Rob Dougan proposed a mix of trip hop beats, orchestral music and electronics. RJD2 began his career as a DJ, but in 2001, began releasing albums under El-P's Def Jux Label.[64] Zero 7's album Simple Things, and in particular, its lead single "Destiny", was regarded highly by underground listeners and achieved significant popularity.[65] In 2006, Gotye debuted his second studio album, Like Drawing Blood. The songs on the album featured down-tempo hip-hop beats and dub style bass reminiscent of trip hop.[66] Hip hop groups Zion I and the Dub Pistols also displayed heavy trip hop influence.[67][68] Norwegian singer and songwriter Kate Havnevik is a classically trained musician, but also incorporates trip hop into her work.[69] During the late 1990s and early 2000s trip hop achieved crossover success in the United States, often lumped under the "electronica" label. Trip hop songs were featured in film soundtracks of this era such as the Matrix series. Many producers who were not explicitly trip-hop artists also displayed its influence during this time. Daniel Nakamura, aka Dan the Automator, released two albums that were heavily inspired by trip hop. His 2000 album Deltron 3030[70] was a concept album about a rapper from the future, portrayed by Del the Funky Homosapien. 2001 saw the release of his side project, Lovage and the album Music to Make Love to Your Old Lady By,[71] with special guests Mike Patton, Prince Paul, Maseo, Damon Albarn, and Afrika Bambaataa. British producer Fatboy Slim's breakthrough album, Halfway Between the Gutter and the Stars,[72] was his most commercially successful release.[importance of example(s)?] Another heavily trip-hop influenced band, Elsiane, published their first album Hybrid in 2007, creating a "mellow, hypnotic atmosphere utilized in the ’90s by big names like Massive Attack, Portishead, etc."[73]
From Wikipedia, the free encyclopediaSnowboardingA snowboarder making a turn in fresh snowFirst played	1979, Muskegon, Michigan, U.S.CharacteristicsType	OutdoorEquipment	Snowboard, bindings, bootsPresenceOlympic	1998Paralympic	2014Snowboarding is a recreational and competitive activity that involves descending a snow-covered surface while standing on a snowboard that is almost always attached to a rider's feet. It features in the Winter Olympic Games and Winter Paralympic Games.Snowboarding was developed in the United States, inspired by skateboarding, sledding, surfing, and skiing. It became popular around the world and was introduced as a Winter Olympic Sport at Nagano in 1998[1] and featured in the Winter Paralympics at Sochi in 2014.[2] As of 2015, its popularity (as measured by equipment sales) in the United States peaked in 2007 and has been in a decline since.[3][4] HistorySnowboarding in Valfréjus, FranceSnowboarder riding off of a corniceFreeride snowboarding, in areas off of the main trailsThe first snowboards were developed in 1965 when Sherman Poppen, an engineer in Muskegon, Michigan, invented a toy for his daughters by fastening two skis together and attaching a rope to one end so he would have some control as they stood on the board and glided downhill. Dubbed the "snurfer" (combining snow and surfer) by his wife Nancy, the toy proved so popular among his daughters' friends that Poppen licensed the idea to a manufacturer, Brunswick Corporation, that sold about a million snurfers over the next decade.[5] And, in 1966 alone, over half a million snurfers were sold.[6] Modern snowboarding was pioneered by Tom Sims and Jake Burton Carpenter, who both contributed significant innovations and started influential companies. In February 1968, Poppen organized the first snurfing competition at a Michigan ski resort that attracted enthusiasts from all over the country.[7] One of those early pioneers was Tom Sims, a devotee of skateboarding (a sport born in the 1950s when kids attached roller skate wheels to small boards that they steered by shifting their weight). In the 1960s, as an eighth grader in Haddonfield, New Jersey, Sims crafted a snowboard in his school shop class by gluing carpet to the top of a piece of wood and attaching aluminum sheeting to the bottom.[8] He produced commercial snowboards in the mid-70s including the Skiboard (also known as the Lonnie Toft flying banana) a molded polyethylene bottom with a Lonnie Toft signature skateboard deck attached to the top.[9][10] Others experimented with board-on-snow configurations at this time, including Welsh skateboard enthusiasts Jon Roberts and Pete Matthews developed their own snowboards to use at their local dry ski slope.[11][12] Also during this same period, in 1977, Jake Burton Carpenter, a Vermont native who had enjoyed snurfing since the age of 14, impressed the crowd at a Michigan snurfing competition with bindings he had designed to secure his feet to the board. That same year, he founded Burton Snowboards in Londonderry, Vermont.[13] The "snowboards" were made of wooden planks that were flexible and had water ski foot traps. Very few people picked up snowboarding because the price of the board was considered too high at $38 and were not allowed on many ski hills, but eventually Burton would become the biggest snowboarding company in the business.[14] Burton's early designs for boards with bindings became the dominant features in snowboarding.The first competitions to offer prize money were the National Snurfing Championship, held at Muskegon State Park in Muskegon, Michigan.[15] In 1979, Jake Burton Carpenter came from Vermont to compete with a snowboard of his own design. There were protests about Jake entering with a non-snurfer board. Paul Graves, and others, advocated that Jake be allowed to race. A "modified" "Open" division was created and won by Jake as the sole entrant. That race was considered the first competition for snowboarding and is the start of what became competitive snowboarding. Ken Kampenga, John Asmussen and Jim Trim placed first, second and third respectively in the Standard competition with best two combined times of 24.71, 25.02 and 25.41; and Jake Carpenter won prize money as the sole entrant in the "open" division with a time of 26.35.[16] In 1980 the event moved to Pando Winter Sports Park near Grand Rapids, Michigan because of a lack of snow that year at the original venue.[17][18] In the early 1980s, Aleksey Ostatnigrosh and Alexei Melnikov, two Snurfers from the Soviet Union, patented design changes to the Snurfer to allow jumping by attaching a bungee cord, a single footed binding to the Snurfer tail, and a two-foot binding design for improved control.[19][20][21] As snowboarding became more popular in the 1970s and 1980s, pioneers such as Dimitrije Milovich (founder of Winterstick out of Salt Lake City, UT), Jake Burton Carpenter (founder of Burton Snowboards from Londonderry, Vermont), Tom Sims (founder of Sims Snowboards), David Kemper (founder of Kemper Snowboards) and Mike Olson (founder of Gnu Snowboards) came up with new designs for boards and mechanisms that slowly developed into the snowboards and other related equipment.[22] From these developments, modern snowboarding equipment usually consists of a snowboard with specialized bindings[23] and boots.[24] In April 1981, the "King of the Mountain" Snowboard competition was held at Ski Cooper in Colorado.[25] Tom Sims along with an assortment of other snowboarders of the time were present.[26] One entrant showed up on a homemade snowboard with a formica bottom that turned out to not slide so well on the snow.In 1982, the first USA National Snowboard race was held near Woodstock, Vermont, at Suicide Six. The race, organized by Graves, was won by Burton's first team rider Doug Bouton.[27] In 1983, the first World Championship halfpipe competition was held at Soda Springs, California. Tom Sims, founder of Sims Snowboards, organized the event with the help of Mike Chantry, a snowboard instructor at Soda Springs.[28] In 1985, the first World Cup was held in Zürs, Austria,[29] further cementing snowboarding's recognition as an official international competitive sport.In 1990, the International Snowboard Federation (ISF) was founded to provide universal contest regulations.[30] In addition, the United States of America Snowboard Association (USASA) provides instructing guidelines and runs snowboard competitions in the U.S. today, high-profile snowboarding events like the Winter X Games, Air & Style, US Open, Olympic Games and other events are broadcast worldwide. Many alpine resorts have terrain parks.At the 1998 Winter Olympic Games in Nagano, Japan, Snowboarding became an official Olympic event.[31] France's Karine Ruby was the first ever to win an Olympic gold medal for Woman's Snowboarding at the 1998 Olympics, while Canadian Ross Rebagliati[32] was the first ever to win an Olympic gold medal for Men's Snowboarding.Initially, ski areas adopted the sport at a much slower pace than the winter sports public. Indeed, for many years, there was animosity between skiers and snowboarders, which led to an ongoing skier vs snowboarder feud.[33] Early snowboards were banned from the slopes by park officials. For several years snowboarders would have to take a small skills assessment prior to being allowed to ride the chairlifts. It was thought that an unskilled snowboarder would wipe the snow off the mountain. In 1985, only seven percent of U.S. ski areas allowed snowboarding,[34] with a similar proportion in Europe. As equipment and skills improved, gradually snowboarding became more accepted. In 1990, most major ski areas had separate slopes for snowboarders. Now, approximately 97% of all ski areas in North America and Europe allow snowboarding, and more than half have jumps, rails and half pipes.In 2004, snowboarding had 6.6 million active participants.[35] An industry spokesman said that "twelve year-olds are out-riding adults." The same article said that most snowboarders are 18–24 years old and that women constitute 25% of participants.There were 8.2 million snowboarders in the US and Canada for the 2009–2010 season. There was a 10% increase over the previous season, accounting for more than 30% of all snow sports participants.[36] On 2 May 2012, the International Paralympic Committee announced that adaptive snowboarding (dubbed "para-snowboarding") would debut as a men's and women's medal event in the 2014 Paralympic Winter Games taking place in Sochi, Russia.[37] StylesSince snowboarding's inception as an established winter sport, it has developed various styles, each with its own specialized equipment and technique. The most common styles today are: freeride, freestyle, and freecarve/race. These styles are used for both recreational and professional snowboarding. While each style is unique, there is overlap between them.JibbingMain article: Jibbing"Jibbing" is the term for technical riding on non-standard surfaces. The word "jib" is both a noun and a verb, depending on the usage of the word. As a noun: a jib includes metal rails, boxes, benches, concrete ledges, walls, vehicles, rocks and logs. As a verb: to jib is referring to the action of jumping, sliding, or riding on top of objects other than snow.[38] It is directly influenced by grinding a skateboard. Jibbing is a freestyle snowboarding technique of riding. Typically jibbing occurs in a snowboard resort park but can also be done in urban environments.Freeriding snowboardingFreeridingMain article: Freeriding (sport)Freeriding is a style without a set of governing rules or set course, typically on natural, un-groomed terrain. The basic allows for various snowboarding styles in a fluid motion and spontaneity through naturally rugged terrain. It can be like freestyle with the exception that no man-made features are utilized. See also Backcountry snowboarding.Freestyle snowboardingFreestyleFreestyle snowboarding is any riding that includes performing tricks. In freestyle, the rider utilizes natural and man-made features such as rails, jumps, boxes, and innumerable others to perform tricks. It is a popular all-inclusive concept that distinguishes the creative aspects of snowboarding, in contrast to a style like alpine snowboarding.Alpine snowboardingAn Alpine snowboarder executes a heel-side carved turn, the typical style in alpine snowboarding.Duration: 12 seconds.0:12Video of a snowboarder practicing carving on a hard slope, equipped with a boardercross board and hard boots.Alpine snowboarding is a discipline within the sport of snowboarding. It is practiced on groomed pistes. It has been an Olympic event since 1998.Freestyle snowboarder films for movie on handrail in UtahSometimes called freecarving or hardbooting(due to the equipment used), this discipline usually takes place on hard packed snow or groomed runs(although it can be practiced in any and all conditions) and focuses on carving linked turns, much like surfing or longboarding, and is seen as superior to other disciplines in many Europeans countries.[according to whom?] Little or no jumping takes place in this discipline. Alpine Snowboarding consists of a small portion of the general snowboard population, that has a well connected social community and its own specific board manufacturers, most situated in Europe. Alpine Snowboard equipment includes a ski-like hardshell boot and plate binding system with a true directional snowboard that is stiffer and narrower to manage linking turns with greater forces and speed.[39] Shaped skis can thank these "freecarve" snowboards for the cutting-edge technology leading to their creation.[40] A skilled alpine snowboarder can link numerous turns into a run placing their body very close to the ground each turn, similar to a motocross turn or waterski carve. Depending on factors including stiffness, turning radius and personality this can be done slowly or fast. Carvers make perfect half-circles out of each turn, changing edges when the snowboard is perpendicular to the fall line and starting every turn on the downhill edge. Carving on a snowboard is like riding a roller coaster, because the board will lock into a turn radius and provide what feels like multiple Gs of acceleration.[41] Alpine snowboarding shares more visual similarities with skiing equipment than it does with snowboarding equipment.[42] Compared to freestyle snowboarding gear:[43] boards are narrower, longer, and stiffer to improve carving performanceboots are made from a hard plastic shell, making it flex differently from a regular snowboard boot and is designed differently to ski boots although they look similar.bindings have a bail or step-in design and are sometimes placed on suspension plates to provide a layer of isolation between an alpine snowboarder and the board, to decrease the level of vibrations felt by the rider, creating a better overall experience when carving, and to give extra weight to the board among other uses.Snowboarder in Tannheim, Tyrol, AustriaSlopestyleMain article: SlopestyleCompetitors perform tricks while descending a course, moving around, over, across, up, or down terrain features. The course is full of obstacles including boxes, rails, jumps, jibs, or anything else the board or rider can slide across. Slopestyle is a judged event and winning a slopestyle contest usually comes from successfully executing the most difficult line in the terrain park while having a smooth flowing line of difficult, mistake-free tricks performed on the obstacles. However, overall impression and style can play a factor in winning a slopestyle contest and the rider who lands the hardest tricks will not always win over the rider who lands easier tricks on more difficult paths.Big airMain article: Big airSebastien Toutant at the downtown Québec big air competitionSnowboarder in the halfpipeBig air competitions are contests where riders perform tricks after launching off a man-made jump built specifically for the event.[44] Competitors perform tricks in the air, aiming to attain sizable height and distance, all while securing a clean landing. Many competitions also require the rider to do a complex trick. Not all competitions call for a trick to win the gold; some intermittent competitions are based solely on height and distance of the launch of the snowboarder. Some competitions also require the rider to do a specific trick to win the major prize.[45] One of the first snowboard competitions where Travis Rice attempted and landed a "double back flip backside 180" took place at the 2006 Red Bull Gap Session.[46] Half-pipeMain article: Half-pipeThe half-pipe is a semi-circular ditch dug into the mountain or purpose-built ramp made up of snow, with walls between 8 and 23 feet (7.0 m). Competitors perform tricks while going from one side to the other and while in the air above the sides of the pipe.Snowboard crossMain article: Snowboard crossSnowboard cross, also known as "boardercross", "boarder X", or "snowboard X", and commonly abbreviated as "SBX", or just "BX", is a snowboarding discipline consisting of several (typically 4 to 6) riders racing head-to-head down a course with jumps, berms and other obstacles constructed out of snow. Snowboard cross began in the 1980s, earning its place as an official Winter Olympic event in the 2006 Turin games. Unlike other snowboard racing disciplines such as parallel giant slalom, competitors race on a single course together.Snowboard racingMain article: Snowboard racingIn snowboard racing, riders must complete a downhill course constructed of a series of turning color indicators (gates) placed in the snow at prescribed distances apart. A gate consists of a tall pole and a short pole, connected by a triangular panel. The racer must pass around the short side of the gate, passing the long side of the gate doesn't count. There are 3 main formats used in snowboard racing including single person, parallel courses or multiple people on the course at the same time (SBX).
From Wikipedia, the free encyclopediaA cast-iron skilletHeavy-duty cookware made of cast iron is valued for its heat retention, durability, ability to maintain high temperatures for longer time duration, and non-stick cooking when properly seasoned. Seasoning is also used to protect bare cast iron from rust. Types of cast-iron cookware include frying pans, dutch ovens, griddles, waffle irons, flattop grills, panini presses, crepe makers, deep fryers, tetsubin, woks, potjies, and karahi.HistoryThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (February 2021) (Learn how and when to remove this template message) An American cast-iron Dutch oven, 1896In Asia, particularly China, India, Korea and Japan, there is a long history of cooking with cast-iron vessels. The first mention of a cast-iron kettle in English appeared in 679 or 680, though this wasn't the first use of metal vessels for cooking. The term pot came into use in 1180. Both terms referred to a vessel capable of withstanding the direct heat of a fire.[1] Cast-iron cauldrons and cooking pots were valued as kitchen items for their durability and their ability to retain heat evenly, thus improving the quality of cooked meals.In Europe and the United States, before the introduction of the kitchen stove in the middle of the 19th century, meals were cooked in the hearth, and cooking pots and pans were either designed for use in the hearth, or to be suspended within it. Cast-iron pots were made with handles to allow them to be hung over a fire, or with legs so that they could stand in the coals. In addition to Dutch ovens with three or four feet, which Abraham Darby I secured a patent in 1708 to produce,[2] a commonly used cast-iron cooking pan called a spider had a handle and three legs allowing it to stand upright over campfires as well as in the coals and ashes of a fireplace.Cooking pots and pans with legless, flat bottoms came into use when cooking stoves became popular; this period of the late 19th century saw the introduction of the flat cast-iron skillet.Cast-iron cookware was especially popular among homemakers during the first half of the 20th century. It was a cheap, yet durable cookware. Most American households had at least one cast-iron cooking pan. Popular manufacturers included Griswold, which began production in 1865, Wagner in 1891, and Blacklock Foundry in 1896. The 20th century also saw the introduction and popularization of enamel-coated cast-iron cookware.Cast iron fell out of favor in the 1960s and 1970s, as teflon-coated aluminum non-stick cookware was introduced and quickly became the item of choice in many kitchens. The decline in daily use of cast-iron cookware contributed to the closure of nearly all the iron cookware manufacturers in the United States. Many went out of business in the 1920s as seen in the List of cast-iron cookware manufacturers. Others were absorbed by other cookware manufacturers.Today, of the large selection of cookware that can be purchased from kitchen suppliers, cast iron comprises only a small fraction. However, the durability and reliability of cast iron as a cooking tool has ensured its survival. Cast-iron pots and pans from the 19th and 20th century continue to see daily use to the present day. They are also highly sought after by antique collectors and dealers.[3] Cast iron has also seen a resurgence of its popularity in specialty markets. Through cooking shows, celebrity chefs have brought renewed attention to traditional cooking methods, especially the use of cast iron.[4] In the 2010s, small startup companies began producing cast-iron cookware designs for specialty cooking markets.SurfaceBare cast ironCast iron's ability to withstand and maintain very high cooking temperatures makes it a common choice for searing or frying, and its excellent heat retention makes it a good option for long-cooking stews or braised dishes.[5] Because cast-iron skillets can develop a "non-stick" surface when cared for properly, they are excellent for frying potatoes or preparing stir-fries. Some cooks consider cast iron a good choice for egg dishes, while others feel the iron adds an off-flavor to eggs. Other uses of cast-iron pans include baking, for instance for making cornbread, cobblers and cakes.[citation needed] Most bare cast-iron pots and pans are cast as a single piece of metal, including the handle. This allows them to be used on both the stovetop and in the oven. Many recipes call for the use of a cast-iron skillet or pot, especially so that the dish can be initially seared or fried on the stovetop then transferred into the oven, pan and all, to finish baking.[6] Likewise, cast-iron skillets can double as baking dishes. This differs from many other cooking pots, which have varying components that may be damaged by the excessive temperatures of 400 °F (200 °C) or more.[citation needed] Cast iron is a poor heat conductor compared to copper and aluminum, and this can result in uneven heating if a cast-iron pan is heated too quickly or on an undersized burner.[7] Cast iron has a higher heat capacity than copper but a lower heat capacity than stainless steel or aluminum.[8] However, cast iron is denser than aluminum and stores more heat per unit volume. Additionally, cast-iron pans are typically thicker than similar sized pans of other materials. The combination of these factors results in cast-iron pans being capable of storing more heat longer than copper, aluminum, or stainless steel pans.[9] Slow heating over an appropriate sized burner (or in an oven) can lead to a more even temperature distribution. Due to the thermal mass of cast-iron utensils, especially heavy duty pots and pans, they can retain heat for a long time, and continue cooking food after the heat source has been removed.Enameled cast ironAn enameled cast-iron potEnameled cast iron is cast iron that has a vitreous enamel glaze applied to the surface. The fusion of the glaze with the cast iron prevents rusting, eliminates the need to season the metal, and allows more thorough cleaning.[10] Enameled cast iron is excellent for slow cooking and drawing flavor from foods.[11] Furthermore, cadmium pigments used in the enameling process are resistant to temperatures of 1,700 to 2,300 °F (900 to 1,280 °C) and can produce vibrant colors.[12] While enamel-coated cast iron does not have the seasoning and cleaning issues of bare cast iron, a similar style of enamel-coated cast iron can cost three or four times its bare cast-iron counterpart. For those seeking to reduce iron in their diet, enameled cast iron limits the leaching of dietary iron into food. However, some of the benefits of bare cast iron, such as the ability to withstand searing heat and resist sticking, are lost through enameling. In addition, chipping of the enamel coating can occur if the pan is dropped, overheated, or cold water is added to a hot pot.[citation needed] Some popular brands of enameled cookware include Le Creuset, Descoware, Cousances, and Druware.SeasoningMain article: Seasoning (cookware)A seasoned pan has a stick-resistant coating created by polymerized oils and fats.[13] Seasoning is a process by which a layer of animal fat or vegetable oil is applied and cooked onto cast-iron or carbon steel cookware.[14] A proper cast iron seasoning protects the cookware from rusting, provides a non-stick surface for cooking, and reduces food interaction with the iron of the pan.[15] Enamel-coated cast-iron pans prevent rust but may need seasoning in some cases.[16] Experts advise against placing a seasoned pan in a conventional dishwasher.[17][18] While some food writers advise against all use of detergent for seasoned pans, tests by America's Test Kitchen found that small amounts of soap do not damage the seasoning.[19] Exposure to acidic foods, such as tomatoes, damages the seasoning and the cookware may need to be re-seasoned again over time.[20] Though some writers recommend completely avoiding cooking acidic foods in seasoned pans, America's Test Kitchen found that cooking acidic foods for short periods of time had no noticeable effect.[19] CleaningBecause other cookware cleaning techniques like scouring or washing in a dishwasher can remove or damage the seasoning on a bare cast-iron pan, experts advise not cleaning these pans like most other cookware. Some chefs advocate simply wiping them out after use, or washing them with hot water and a stiff brush.[21] Others advocate washing with mild soap and water, and then re-applying a thin layer of fat or oil.[22] A third approach is to scrub with coarse salt and a paper towel or clean rag.[23] Health effectsAn American Dietetic Association study found that cast-iron cookware can leach significant amounts of dietary iron into food. The amounts of iron absorbed varied greatly depending on the food, its acidity, its water content, how long it was cooked, and how old the cookware is. The iron in spaghetti sauce increased 845 percent (from 0.61 mg/100 g to 5.77 mg/100 g), while for other foods it increased less dramatically; for example, the iron in cornbread increased 28 percent, from 0.67 to 0.86 mg/100 g.[24] Anemics, and those with iron deficiencies, may benefit from this effect,[25] which was the basis for the development of the lucky iron fish, an iron ingot used during cooking to provide dietary iron to those with iron deficiency. People with hemochromatosis (iron overload, bronze disease) should avoid using cast-iron cookware because of the iron leaching effect into the food.[26] Laboratory tests conducted by America's Test Kitchen found that an unseasoned cast-iron skillet leached significant iron into tomato sauce (10.8 mg/100 g) while a seasoned cast-iron pan leached only a small amount.[19]
A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs[1][2][3] makes them applicable to tasks such as unsegmented, connected handwriting recognition[4] or speech recognition.[5][6] The term "recurrent neural network" is used to refer to the class of networks with an infinite impulse response, whereas "convolutional neural network" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[7] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.[8] HistoryThe Ising model (1925) by Wilhelm Lenz[9] and Ernst Ising[10][11] was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972.[12][13] This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986.[14] In 1993, a neural history compressor system solved a "Very Deep Learning" task that required more than 1000 subsequent layers in an RNN unfolded in time.[15] LSTMLong short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.[16] Around 2007, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications.[17] In 2009, a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition.[18][19] In 2014, the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset[20] benchmark without using any traditional speech processing methods.[21] LSTM also improved large-vocabulary speech recognition[5][6] and text-to-speech synthesis[22] and was used in Google Android.[18][23] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49%[citation needed] through CTC-trained LSTM.[24] LSTM broke records for improved machine translation,[25] Language Modeling[26] and Multilingual Language Processing.[27] LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.[28] ArchitecturesMain article: Layer (deep learning)RNNs come in many variants.Fully recurrentCompressed (left) and unfolded (right) basic recurrent neural network Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in "layers" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'. It is "unfolded" in time to produce the appearance of layers.Elman networks and Jordan networksThe Elman networkAn Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one.[29] At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.[29] Elman and Jordan networks are also known as "Simple recurrent networks" (SRN).Elman network[30]ℎ�=�ℎ(�ℎ��+�ℎℎ�−1+�ℎ)��=��(��ℎ�+��){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}} Jordan network[31]ℎ�=�ℎ(�ℎ��+�ℎ��−1+�ℎ)��=��(��ℎ�+��){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}} Variables and functions��{\displaystyle x_{t}}: input vectorℎ�{\displaystyle h_{t}}: hidden layer vector��{\displaystyle y_{t}}: output vector�{\displaystyle W},�{\displaystyle U} and�{\displaystyle b}: parameter matrices and vector �ℎ{\displaystyle \sigma _{h}} and��{\displaystyle \sigma _{y}}: Activation functions HopfieldMain article: Hopfield networkThe Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning, then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.Bidirectional associative memoryMain article: Bidirectional associative memory Introduced by Bart Kosko,[32] a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.[33] A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.[34] Echo stateMain article: Echo state networkEcho state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series.[35] A variant for spiking neurons is known as a liquid state machine.[36] Independently RNN (IndRNN)The independently recurrent neural network (IndRNN)[37] addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections.RecursiveMain article: Recursive neural networkA recursive neural network[38] is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation.[39][40] They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing.[41] The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.[42] Neural history compressorThe neural history compressor is an unsupervised stack of RNNs.[43] At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.The system effectively minimizes the description length or the negative logarithm of the probability of the data.[44] Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.It is possible to distill the RNN hierarchy into two RNNs: the "conscious" chunker (higher level) and the "subconscious" automatizer (lower level).[43] Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn, this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.[43] A generative model partially overcame the vanishing gradient problem[45] of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a "Very Deep Learning" task that required more than 1000 subsequent layers in an RNN unfolded in time.[15] Second order RNNsSecond-order RNNs use higher order weights����{\displaystyle w{}_{ijk}} instead of the standard ���{\displaystyle w{}_{ij}} weights, and states can be a product. This allows a direct mapping to a finite-state machine both in training, stability, and representation.[46][47] Long short-term memory is an example of this but has no such formal mappings or proof of stability.Long short-term memoryMain article: Long short-term memoryLong short-term memory unitLong short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called "forget gates".[48] LSTM prevents backpropagated errors from vanishing or exploding.[45] Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks[18] that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved.[49] LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components.Many applications use stacks of LSTM RNNs[50] and train them by connectionist temporal classification (CTC)[51] to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.[52] Gated recurrent unitMain article: Gated recurrent unitGated recurrent unitGated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants.[53][54] Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory.[55] They have fewer parameters than LSTM, as they lack an output gate.[56] Bi-directionalMain article: Bidirectional recurrent neural networks Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs, one processing the sequence from left to right, and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs.[57][58] Continuous-timeA continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs.For a neuron�{\displaystyle i} in the network with activation ��{\displaystyle y_{i}}, the rate of change of activation is given by: ���˙�=−��+∑�=1�����(��−Θ�)+��(�){\displaystyle \tau _{i}{\dot {y}}_{i}=-y_{i}+\sum _{j=1}^{n}w_{ji}\sigma (y_{j}-\Theta _{j})+I_{i}(t)} Where:��{\displaystyle \tau _{i}} : Time constant of postsynaptic node ��{\displaystyle y_{i}} : Activation of postsynaptic node �˙�{\displaystyle {\dot {y}}_{i}} : Rate of change of activation of postsynaptic node ���{\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node �(�){\displaystyle \sigma (x)} : Sigmoid of x e.g.�(�)=1/(1+�−�){\displaystyle \sigma (x)=1/(1+e^{-x})}.��{\displaystyle y_{j}} : Activation of presynaptic node Θ�{\displaystyle \Theta _{j}} : Bias of presynaptic node ��(�){\displaystyle I_{i}(t)} : Input (if any) to node CTRNNs have been applied to evolutionary robotics where they have been used to address vision,[59] co-operation,[60] and minimal cognitive behaviour.[61] Note that, by the Shannon sampling theorem, discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations.[62] This transformation can be thought of as occurring after the post-synaptic node activation functions ��(�){\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling.Hierarchical recurrent neural network[icon]This section needs expansion. You can help by adding to it. (August 2019) Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.[43][63] Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.[64] Hierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions. Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods.[65] Recurrent multilayer perceptron networkGenerally, a recurrent multilayer perceptron network (RMLP network) consists of cascaded subnetworks, each containing multiple layers of nodes. Each subnetwork is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed-forward connections.[66] Multiple timescales modelA multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization depending on the spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties.[67][68] With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence.[citation needed] Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.[64][69] Neural Turing machinesMain article: Neural Turing machineNeural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources which they can interact with by attentional processes. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.[70] Differentiable neural computerMain article: Differentiable neural computer Differentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology.Neural network pushdown automataNeural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analog stacks that are differentiable and trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).[71] Memristive networksGreg Snider of HP Labs describes a system of cortical computing with memristive nanodevices.[72] The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures that may be based on memristive systems. Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. The evolution of these networks can be studied analytically using variations of the Caravelli–Traversa–Di Ventra equation.[73]